---
layout: single
title:  "Distributed Bayesian Reasoning - Bayesian Averaging"
slug: distributed-bayesian-reasoning-bayesian-averaging
date:   2021-11-22 00:00:00 +0200
toc: true
weight: 112
series: ['Distributed Bayesian Reasoning']

---

This article is part of the series on distributed Bayesian reasoning. It assumes you have read the previous article on the [Basic Math](/distributed-bayesian-reasoning-math).

<!--more--> 

## The Problem: Small Samples

In [Basic Math](/distributed-bayesian-reasoning-math), we used the informed probability $ P_i $ as the prior beliefs of the meta-reasoner. We calculated this as the ratio:

$$
    P_i(ğ´=1|ğµ=1) = \frac{P(ğ´=1,ğµ=1|Bâ‰¥0)}{P(ğµ=1|B â‰¥ 0)} = \frac{c(ğ´=1,ğµ=1)}{c(ğµ=1) }
$$

But this is actually a little Naive. Suppose only one person actually voted on both ğ´ and ğµ and accepts both. Then $ğ‘(ğ´=1, ğµ=1) = ğ‘(ğµ=1) = 1$, and this ratio is 100%. Is this really a good estimate of the probability that the meta-reasoner would accept ğ´ given they accepted ğµ?

Certainly not. A single vote from a single user is not a great deal of information. We need a more sophisticated way of estimating the priors of the meta-reasoner based on the evidence we have in the form of arguments and votes. 

The Bayesian approach to answering this question requires us to have **priors**: we actually need to **start** with an estimate of this probability -- or rather, a distribution of possible probabilities -- even before we have any data! Then we can use the rules of Bayesian belief updating to combine our priors with our data to come up with a posterior belief.

## The Beta-Bernoulli Model

It turns out, we are actually dealing with a textbook example of a problem that can be solved with a simple Bayesian hierarchical model. The solution, using a beta-Bernoulli distribution, is amply described elsewhere (I learned about them from [this book](https://www.amazon.com/Doing-Bayesian-Data-Analysis-Tutorial/dp/0124058884)). Here is the solution:

Let:

- Ï‰ = our **prior** estimate of the probability that the average juror accepts ğ´ before getting any vote data
- Îº = our **prior** estimate of the concentration of likely values around Ï‰ (high Îº means low variance)
- ğ‘ = $c(A >= 0)$ = the number of users who have voted on ğ´
- z = $c(A=1)$ = the number of those users who also agree with ğ´

Then our **posterior** estimate of the probability that the average user accepts ğ´ is given they have voted on it is:

$$
\label{0}
\begin{aligned}
    \frac{Ï‰(Îº - 2) + 1 + z}{Îº + N}
\end{aligned} \tag{0}
$$

What should we use as our prior Ï‰? That depends on the context. If this method is being implemented in a social platform, then this can be based on historical data. For example if in the past, the average accept/reject ratio for arguments submitted to the platform was 80%, then having nothing else to go on, 80% is a good estimate of Ï‰. Our estimate of Îº can also be made using historical data.

What we have done here is sometimes called [Bayesian Averaging](https://en.wikipedia.org/wiki/Bayesian_average#:~:text=A%20Bayesian%20average%20is%20a,available%20data%20set%20is%20small.). The above formula essentially gives us a weighted average of our prior Ï‰ and the observed ratio z/ğ‘, with our data z/ğ‘ getting higher weight the larger the value of N relative to Îº.



## The Bayesian-Average Probability Function

When calculating values of ğ‘ƒ up to this point, we have just taking ratios of counts from our votes table (the ğ‘ function). For example, the formula for ğ‘ƒ(ğ´=a) is just:

$$
    P(ğ´=a) = \frac{c(ğ´=a)}{c()}
$$

Where c() is the total number of voters. To use a Bayesian approach to estimating probabilities, instead of taking a ratio, we plug these same two counts into $\eqref{0}$.

Let's define a new function ğ‘ƒáµ¥ that does this for us.

So where, by definition

$$
    P(Î±) = \frac{c(Î±)}{c()}
$$

We have instead:

$$
    P_v(Î±) = \frac{Ï‰(Îº - 2) + 1 + c(Î±)}{Îº + c()}
\tag{1}\label{1}
$$

And where by definition of conditional probability:

$$
    P(Î±|Î²) = \frac{c(Î±,Î²)}{c(Î²)}
$$

We have instead

$$
    P_v(Î± |\vert Î²) = \frac{Ï‰(Îº - 2) + 1 + c(Î±,Î²)}{Îº + c(Î²)}
\tag{2}\label{2}
$$    

Now let's compute an actual value of ğ‘ƒáµ¥(ğ´=1). First, we need to choose priors. Let's suppose that historically, on average 80% voters accept root claims initially. So Ï‰=80%. And let's suppose the variation in this distribution can be represented by Îº=10. So

$$
\begin{aligned}
    P_v(A=1) &= \frac{Ï‰(Îº - 2) + 1 + c(A=1)}{Îº + c()}\cr
             &= \frac{(80\\%)(10-2) + 1 + 500}{10 + 1000} â‰Š 50.23\\%
\end{aligned}
$$



In this case, the large amount of votes overwhelms our relatively weak prior, and so our result is very close to $ğ‘ƒáµ¢(ğ´=a) = 50\\%$.

## Two-Level Bayesian Averaging

Reviewing where we are going with this, recall from the [Basic Math](/distributed-bayesian-reasoning-math) article that the justified opinion formula in the case of an argument tree with a single premise argument is:


$$
\label{3}
P_h(ğ´=1) = \sum_{b=0}^{1} P_i(ğ´=1|ğµ=b)P_h(ğµ=b)
\tag{3}
$$

Now we are saying that $ ğ‘ƒáµ¢(ğ´=1 \vert ğµ=b) $ may not be a good estimate that the average person would accept/reject A given they accepted/rejected B. So instead, we want to use Bayesian averaging and use the formula for $ ğ‘ƒáµ¥(A=1 \vert B=b)$ in place of $ğ‘ƒáµ¢(ğ´=1 \vert ğµ=b)$. So substituting $\eqref{2}$ into $\eqref{3}$

$$
\label{4}
P_h(ğ´=1) = \sum_{b=0}^{1} \frac{Ï‰(Îº - 2) + 1 + c(A=a,B=b)}{Îº + c(B=b)}P_h(ğµ=b)
\tag{4}
$$


But what are our priors Ï‰ and Îº? 

Recall that we have just used Bayesian averaging to estimate of the probability that the average person accepts ğ´: $ğ‘ƒáµ¥(ğ´=1)=50.23\\%$. This seems like an reasonable prior for our estimate of $ğ‘ƒáµ¥(ğ´=1 \vert ğµ=b)$. Before considering the 150 users who voted on ğµ, we have a large amount of data telling us the average user has a roughly even chance of accepting ğ´, and we have no prior reason to believe that accepting/rejecting ğµ either increases or decreases this probability. Unless we have strong evidence showing accepting/rejecting ğµ changes the probability that people accept/reject ğ´, we should assume it doesn't. 

However if we use $ğ‘ƒáµ¥(ğ´=1)$ as a prior for $ğ‘ƒáµ¥(ğ´=1 \vert ğµ=b)$, there is a subtle problem: we will be "double counting". We are counting votes of users for whom ğ´=1 and ğµ=b as evidence for estimating $ğ‘ƒáµ¥(ğ´=1)$, and then counting the same votes as evidence for estimating $ğ‘ƒáµ¥(ğ´=1 \vert ğµ=b)$. So to avoid double counting, our prior should actually be $ğ‘ƒáµ¥(ğ´=1 \vert ğµâ‰ b)$.

The priors for $ğ‘ƒáµ¥(ğ´=1 \vert ğµâ‰ b)$, on the other hand, can be the same priors we used to calculate $ğ‘ƒáµ¥(ğ´=1)$, because we don't have anything to go on besides historical data. So let's Ï‰=80% and Îº=10. Then let's start with ğµ=1, and calculate:

$$
\begin{aligned}
    P_v(A=1|Bâ‰ 1) &= \frac{Ï‰(Îº - 2) + 1 + c(A=1,Bâ‰ 1)}{Îº + c(Bâ‰ 1)}\cr
                 &= \frac{80\\%(10 - 2) + 1 + 420}{10 + 900} â‰ˆ 46.96%
\end{aligned}
$$

Now we can set $Ï‰=ğ‘ƒáµ¥(ğ´=1 \vert ğµâ‰ 1)$ as the prior for calculating $ğ‘ƒáµ¥(ğ´=1 \vert ğµ=1)$.

What is our prior estimate of Îº? We might think that it should be proportional to the number of people who voted on ğ´, but this is mistaken. A large number of votes on ğ´ provide strong evidence for estimating Ï‰ = ğ‘ƒáµ¥(ğ´=a). But our estimate for Îº is based on our prior expectations about **the degree to which people are influenced by arguments.** This information can come from observation of actual variance in the case of past arguments. If this is historical very high, then Îº should be low, and vice versa. 

For simplicity, let's use the same prior Îº=10 that we used before. 

We can now finally calculate:

$$
\begin{aligned}
    P_v(ğ´=1|B=1)    &= \frac{ğ‘ƒáµ¥(ğ´=1|Bâ‰ 1)(Îº - 2) + 1 + c(A=1,B=1)}{Îº + c(B=1)}\cr
                    &â‰ˆ \frac{(46.96\\%)(10 - 2) + 1 + 80}{10 + 100} â‰ˆ 77.05\\%
\end{aligned}
$$


This is slightly lower than $ğ‘ƒáµ¢(ğ´=1 \vert ğµ=1) = 80%$. This is because we still have a reasonably large number of votes on ğµ, and these votes provide strong evidence for a posterior value of 80% that overpower the prior estimate.

Clearly, we can extend this reasoning to long argument threads, though we will not do this here.


## Further Development

This document is a work in progress -- these models have not been fully developed. In fact, we are looking for collaborators. If you are an expert in Bayesian hierarchical models and causal inference, please contact collaborations@deliberati.io.

