---
layout: single
title:  "Distributed Bayesian Reasoning Math"
date:   2021-10-27 00:00:00 +0200

weight: 110
# toc_sticky: true
series: ['Distributed Bayesian Reasoning']
# categories: ["social-protocols"]

sidebar:
  - title: "In This Series"
    nav: "distributed-bayesian-reasoning"
  - nav: "distributed-bayesian-reasoning-related"
    title: "Related Articles"
  - title: ""
    text: "
    <h5>Sample Argument Used in this Article</h5>
            <img src='/assets/images/distributed-bayesian-reasoning/sample-argument-reference.svg'
                 alt='Simplified Argument'
                 style='display: block; margin-left: 0px; margin-right: 0px; padding-left: 0px; padding-right: 0px; max-width: 350px' />
     "

# image: assets/images/distributed-bayesian-reasoning/jeffrey-conditioning-formula.png


---


In this article we develop the basic mathematical formula for calculating the opinion of the meta-reasoner in arguments involving a single main argument thread.

<!--more-->

## Background Reading

To understand this article you should first read:

- [Introduction to Distributed Bayesian Reasoning](/distributed-bayesian-reasoning-introduction)
- [The Meta-Reasoner](/the-meta-reasoner)

For a deeper understanding of some of the assumptions we make in this article, it may also help to read:

- [The Argument Model](/argument-model)

You also need familiar with basic syntax and concepts from probability theory, specifically:

- the definition of [conditional probability](https://en.wikipedia.org/wiki/Conditional_probability)
- the [law of total probability](https://en.wikipedia.org/wiki/Law_of_total_probability)


## Sample Argument

<!-- {{< figure src="/assets/images/distributed-bayesian-reasoning/sample-argument-reference.svg" title="Sample Argument" >}} -->




<img src='/assets/images/distributed-bayesian-reasoning/sample-argument-reference.svg'
                 alt='Simplified Argument'
                 style='display: block; margin-left: auto; margin-right: auto; max-height: 400px' />

Suppose a sensational murder trial is being discussed in an online platform that allows the general public to vote on what they think the verdict should be and why.

Initially, 1,000 users vote on the root claim (ğ´) *the defendant is guilty*, before any discussion has taken place on the platform. Then after this initial vote, somebody submits an argument claiming (ğµ) *the defendant signed a confession*, and users are asked to vote on this claim. 

150 out of the 1,000 users vote on ğµ. Of these 150 users, a small number changed their vote on ğ´ after voting on ğµ (presumably, because they found ğµ convincing). 

The final votes are tabulated in the following table. We represent votes using the numeric values **0=reject**, **1=accept**, and **-1=didn't vote**. 


|          | A=-1 | A=0 | A=1 |  SUM
| -------- | ---- | --- | --- |  ----
| **B=-1** |  0   | 455 | 395 |  850
| **B=0**  |  0   | 25  | 25  |  50
| **B=1**  |  0   | 20  | 80  |  100
| **SUM**  |  0   | **500** | **500** | **1000**
| **Bâ‰¥0**  |  0   | 45  | 105 |  150


According to this table, all 1,000 users voted on ğ´, with 500 rejecting ğ´ (ğ´=0) and 500 accepting ğ´ (ğ´=1). But only 150 users voted on ğµ (ğµâ‰¥0).

## Raw Probabilities

Our first step is to convert these counts into probabilities

Let's define a function ğ‘ that returns the values of a cell in this table. For example:

$$
\begin{aligned} 
    ğ‘(A=1)     &= 500 \cr
    ğ‘(A=1,B=0) &= 25 \cr
    ğ‘() &= 1000
\end{aligned}
$$


From this, we can define a function ğ‘ƒ that tells us the probability that a random user voted in some way:


$$
    P(A=a) = \frac{ğ‘(A=a)}{c()}
$$

So for example: 

$$
\begin{aligned} 
    P(A=1) &= \frac{ğ‘(A=1)}{c()} \cr
            &= \frac{500}{1000} = 50\\% 
\end{aligned}
$$



We can also define [conditional probabilities](https://en.wikipedia.org/wiki/Conditional_probability), for example the probability that a random user accepts ğ´ given they accept ğµ is:

$$
    P(A=1|B=1) = \frac{P(A=1,B=1)}{P(B=1)} 
$$

We can calculate conditional probabilities just by taking the ratio of counts, because:


$$
\begin{aligned}
    P(A=a|B=b) &= \frac{P(A=a,B=b)}{P(B=b)}\cr \cr
           &= \frac{ğ‘(A=a,B=b) \div c()}{ ğ‘(B=b) \div c() }\cr \cr
           &= \frac{ğ‘(A=a,B=b)}{ğ‘(B=b)} 
\end{aligned}
$$


So for example 


$$
\begin{aligned} 
    P(A=1|B=1) &= \frac{ğ‘(A=1,B=1)}{ğ‘(B=1)} \cr
                &= \frac{80}{100} = 80\\%
\end{aligned}
$$



Note that, $P$ represents the probability that a randomly selected user, **from among those who voted**, votes in some way. If this sample of users is small or biased, $P$ may not be a good estimate of what an average person actually believes. We will ignore this detail in this article, but address it in [Bayesian Averaging](/distributed-bayesian-reasoning-bayesian-averaging).



## Informed Probabilities


$P(A=1)$ is only 50%, but $P(A=1 \vert B=1)$ is 80%. This means that users who accept claim ğµ are more likely to accept claim ğ´. So ğµ apparently is an effective supporting argument for ğ´. On the other hand $P(A=1 \vert B=0)$ = 25/50 = 50%. Users who reject ğµ are not more likely to accept ğ´. 

Notably, among users who either **accept OR reject ğµ**, 70% of users accept ğ´:


$$
\begin{aligned} 
    P(A=1 | B â‰¥ 0) &= \frac{c(A=1, B â‰¥ 0)}{c(B â‰¥ 0)}\cr \cr
        &= \frac{105}{150} = 70\\%
\end{aligned}
$$


While only 50% of users accept ğ´ overall. Apparently simply **voting on ğµ** made users more likely to accept ğ´.

What's happening here is that, among users who voted on $B$, a large number accept $B$ as true, and as we've seen users who accept $B$ are more likely to accept $A$. What makes the group of users who voted on $B$ different is that all of them are **informed about ğµ**.  Whether they accept it as true or not, they have at least been presented with the claim that (ğµ) *the defendant signed a confession* and had a chance to reject it, or to accept it and revise their belief accordingly. This is not necessarily the case for the larger group of users: perhaps the media coverage of the murder never mentioned any confession, and most users never learned about it until they were asked to vote on claim $B$.

This is just made-up data, but it is meant to illustrate something that is often the case in reality: **arguments can change minds** -- **especially if they provide new information**.

Our goal is to calculate the beliefs of the [Meta-Reasoner](/the-meta-reasoner): a hypothetical **fully-informed** user who shares the knowledge of all the other users. So the opinion of users who voted on ğµ is probably a better estimate of a fully-informed opinion.

So we'll call the users who voted on ğµ the **informed users**, and our first step is estimating the beliefs of the meta-reasoner will be to represent the opinion of the average informed user with the **informed probability** function $ P_i $:


$$
    P_i(A) = P(A|Bâ‰¥0)
$$


So 


$$
    P_i(A=1) = ğ‘ƒ(A=1|Bâ‰¥0)
$$


Which we have already calculated to be 70%.


## The Law of Total Probability

The informed opinion on ğ´ depends on 1) the probability that an informed user actually accepts ğµ, and 2) the probability that a user who accepts ğµ also accepts ğ´. In fact we can rewrite the equation for $P_i(A)$ in terms of these probabilities. Since the set of users who accept ğµ and the set that reject ğµ *partition* the set of users who voted on ğµ, the [law of total probability](https://en.wikipedia.org/wiki/Law_of_total_probability) says that:


$\label{1}$

$$
\begin{aligned} 
    P_i(A)    &= ğ‘ƒ(A=1|Bâ‰¥0) \cr
              &= \sum_{bâ‰¥0} P_i(B=b)P_i(A|B=b)\cr \cr
              &=P_i(B=0)P(A|B=0)\cr \cr
              &+P_i(B=1)P(A|B=1)
\end{aligned}
\tag{1}
$$


We have already calculated $P(A=1 \vert B=0)=50\\%$ and $P(A=1 \vert B=1)=80\\%$ above, so it remains only to calculate $P_i(B=1)$:


$$
\begin{aligned} 
    P_i(B=1)
        &= P(B=1|Bâ‰¥0)\cr \cr
        &= \frac{ğ‘(B=1)}{ğ‘(Bâ‰¥0)}\cr  \cr
        &= \frac{100}{150} = 66â…”\\%\cr
\end{aligned}
$$


Plugging these values into $\eqref{1}$, we again get 70%.


$$
\begin{aligned} 
    P_i(A=1) &=P_i(B=0)P(A=1|B=0)\cr \cr
        &+P_i(B=1)P(A=1|B=1) \cr \cr
        &=(1 - 66â…”\\%)(50\\%) + (66â…”\\%)(80\\%) = 70\\%\cr \cr
\end{aligned}
$$


Formula $\eqref{1}$ is important because it shows us exactly how the probability that users accept ğµ determines the probability that they accept ğ´. And critically, it shows us what the probability of accepting ğ´ **would be** if the probability of accepting ğµ were different.

<!--
TODO: show linear function of Pi(A)
-->

## Distributed Reasoning

Now suppose a second group of 10 users holds an argument about whether to accept ğµ, and during this argument users voted on claim ğº, *the signature was forged*. And suppose these users unanimously accept ğº and found it very convincing: only 1/10 users accept ğµ after accepting ğº.

Clearly, the opinion of the meta-reasoner about ğµ will be equal to the opinion of the second group of voters, since this opinion is more informed, reflecting any new information conveyed by ğº.

Let's define a function $P_h$ that gives us the beliefs of the meta-reasoner. The beliefs of the meta-reasoner about $B$ is the informed opinion on $B$, which is the opinion of users who also voted on $G$:


$$
\label{2}
\begin{aligned} 
    P_h(B) &= P(B|G â‰¥ 0)
\end{aligned}
\tag{2}
$$


Let's put the vote counts from the sub-jury in a table:

|          | B=0 | B=1 | B â‰¥ 0
| -------- | --- | --- | -----
| **ğº=0**  | 0  | 0    | 0
| **ğº=1**  | 9  | 1    | 10
| **ğºâ‰¥0**  | **9**  | **1** | **10** 

And now we can calculate:

$$
\begin{aligned} 
    P_h(B=1) &= P(B=1|G â‰¥ 0)  \cr
             &= \frac{c(B=1, Gâ‰¥0)}{c(Gâ‰¥0)} \cr
             &= \frac{1}{10} = 10\\\%
\end{aligned}
$$

Recall that $\eqref{1}$ tells us how belief in $B$ determines the first group of users' belief in $A$. So to calculate the probability that a member of the first jury would accept ğ´ *if they held the beliefs of the second jury about ğµ*, we simply substitute of $ P_h(B=b) $ in place of $ P_i(B=b) $ in $\eqref{1}$:

$$
\label{3}
\begin{aligned} 
    P_h(A) &= \left. \sum_{bâ‰¥0} P_i(B=b)P(A|B=b) \right\vert_{P_i=P_h}\cr \cr
           &= \sum_{bâ‰¥0} P_h(B=b)P(A|B=b) 
\end{aligned}
\tag{3}
$$


Plugging in the numbers:

$$
\begin{aligned} 
    P_h(A=1) =\space&ğ‘ƒ_h(B=0)ğ‘ƒ(A=1|B=0) \cr
            +\space&ğ‘ƒ_h(B=1)ğ‘ƒ(A=1|B=1)\cr \cr
           =&\space (1 - 10\\%)(50\\%) + (10\\%)(80\\%) = 53\\%
\end{aligned}
$$

The meta-reasoner's belief $ğ‘ƒâ‚•(A=1)$ is very close to $ğ‘ƒ(A=1 \vert B=0)=50\\%$ -- the average belief of users who voted on ğµ but rejected it -- because a fully-informed user would probably reject ğµ.



<aside class="custom-aside" markdown="1">

### Front-Door Adjustment

To readers who are familiar with [Judea Pearl](https://en.wikipedia.org/wiki/Judea_Pearl)'s work on [graphical causal models](https://ftp.cs.ucla.edu/pub/stat_ser/r236-3ed.pdf), formula $\eqref{3}$ may look familiar: it is a 
[front-door adjustment](https://medium.data4sci.com/causal-inference-part-xii-front-door-criterion-38bec5172f3e). Given the causal graph (ğºâ‰¥0) â†’ ğµ â†’ ğ´:

$$
\begin{aligned} 
    P_h(A) = P(A|do(Gâ‰¥0)) = \sum_{bâ‰¥0} P_h(b)P(A|b)
\end{aligned}
$$

A derivation of the above equation using the [do-calculus](https://plato.stanford.edu/entries/causal-models/do-calculus.html) is provided in the [Appendix](#derivation-1). The causal graph follows from the conditional independence assumption that we will discuss in the next section. 

Given this causal graph and our probability data, we've used the do-calculus to **simulate an intervention**: we have estimated the probability that a user **would believe** ğ´ if they had voted on all claims ($A$, $B$, and $G$) in the argument, even though no single user has actually done so!


### Jeffrey's Rule

Formula $\eqref{3}$ is also Jeffrey's Rule: the general rule for Bayesian belief revision for situations where new information may come with uncertainty, as described in the Stanford [Stanford Encyclopedia of Philosophy article on Bayes Theorem](https://plato.stanford.edu/entries/bayes-theorem/#4):

> If a person with a prior such that $ 0 < P(B) < 1 $ has a learning experience whose sole immediate effect is to change her subjective probability for $B$ to $P_h(B)$, then her post-learning posterior for any $A$ should be [substituting our own terms]:
>
> $$ \begin{aligned}  ğ‘ƒâ‚•(A)  \cr= &ğ‘ƒ(A \vert B=1)ğ‘ƒâ‚•(B=1) \cr + &ğ‘ƒ(A \vert B=0 )(1 - ğ‘ƒâ‚•(B=1)) \end{aligned} $$
>

Which is of course again $\eqref{3}$ with the terms of the summation expanded. Note the requirements about the "sole immediate effect" requires a conditional independence assumption, discussed in the next section.



</aside>


## Causal Assumptions

### Conditional Independence

Formula $\eqref{3}$ is only valid if we assume the meta-reasoner forms their belief about (ğ´) *the defendant is guilty* entirely based on their belief about (ğµ) *the defendant signed a confession*. So their belief in (ğº) *the signature was forged* does not effect their belief in ğ´ *directly*, but only *indirectly* through ğµ. In other words ğ´ is [**conditionally independent**](https://en.wikipedia.org/wiki/Conditional_independence) of $G$ given ğµ. We discuss the justification for making these causal assumptions in the [Meta-Reasoner](/the-meta-reasoner/#the-causal-model-and-the-justified-opinion).

Unfortunately, we can't make the same sort of assumptions about (ğ¶) *the defendant retracted her confession*. ğ¶ does not effect belief in ğ´ only through ğµ: learning that the defendant retracted her confession may make less of an impression on a user who never believed the defendant signed a confession in the first place. So the effect of accepting ğ¶ on a user's acceptance of ğ´ depends on whether or not that user accepts ğµ.

The reason we can make the conditional independence assumption about ğº and not ğ¶ is that ğº is the premise of a [premise argument](/argument-model#premise-arguments), whereas ğ¶ is the premise of a [warrant argument](/argument-model#warrant-arguments). The difference between premise arguments and warrant arguments is discussed in more detail in the [Argument Model](/argument-model).

<aside class="custom-aside" markdown="1">

### Argument Threads

Below is a graph of our argument using the terminology and notation from the [argument model](/argument-model). The sub-graph in blue comprises a single [argument thread](/argument-model#argument-threads). 


<img src="/assets/images/distributed-bayesian-reasoning/sample-argument-reference-with-notation.svg"
     alt="Graph of Sample Argument Used in This Article"
     style="display: block; margin-left: auto; margin-right: auto; max-height: 450px" />


As discussed in the section [Argument Threads are Dialogs](/argument-model/#argument-threads-are-dialogs) in the argument model, each argument in the argument thread should be interpreted as a claim that the premise of that argument, given acceptance of the premises of any preceding arguments in the thread, is a good reason to accept or reject the root claim. So rather than being conditionally independent, acceptance of ğ´ given ğµ clearly depends on whether the user accepts ğ¶

So while we can create an independent sub-jury to decide whether or not the meta-reasoner accepts ğ¶, we can't use an independent sub-jury to decide how their acceptance of ğ¶ effects their acceptance of ğ´. We need the main jury to consider the premises of the entire argument thread together, and tell us the probability that the average user -- and thus the meta-reasoner -- would accept ğ´ given they accept the premises ğµ and ğ¶. 


</aside>

<style>
.custom-aside
{
  margin: auto;
  background-color: lightgrey;
  border: 1px solid black;
  max-width: 600px;
  padding-top: 1em;
  padding-bottom: 0px;
  padding-left: 1em;
  padding-right: 1em;
  margin-bottom:  1em;
}

aside h3 {
    margin-top: 0px;
}

</style>


## Formula for a 2-Argument Thread

Our next task is to calculate the opinion of the meta-reasoner after argument ğ¶ has been made. 

First, we need to update our definition of the informed opinion. Previously, we defined the informed opinion as the opinion of users who voted on ğµ; now that we have a second premise ğ¶ in the [argument thread](/argument-model#argument-threads), we should include ğ¶ in the definition of informed opinion.

However, for users who reject ğµ, what they think about ğ¶ is irrelevant, because (ğ¶) *the defendant retracted her confession* is only argued as a way of convincing people who accept (ğµ) *the defendant signed a confession* that they still shouldn't accept ğ´. We discuss this important concept in the the section [Argument Threads are Dialogs](/argument-model#argument-threads-are-dialogs) in the argument model.

So we'll define the informed opinion as the opinion of users who either reject ğµ, or accept ğµ and have voted on ğ¶:

$$
\begin{aligned} 
    &P_i(A)    = P(A | (B=0 âˆ¨ (B=1 âˆ§ Câ‰¥0)) )
\end{aligned}
$$

We can then rewrite the formula for $ P_i(A) $ using the law of total probability and some probability calculus. The derivation is similar to the derivation of $\eqref{1}$ and is shown in the [appendix](#derivation-2):



$$
\label{4}
    \begin{aligned} 
    P_i(A)    =~ &P_i(B=0) P(A|B=0) \cr
                &\begin{aligned}
                   +~ P_i(B=1)  &\sum_{câ‰¥0} P_i(C=c|B=1)  \cr
                                &Ã— P(A|B=1,C=c)
                \end{aligned}
    \end{aligned}
    \tag{4}
$$

Now, suppose a third sub-jury holds a sub-trial about whether to accept ğ¶, giving us $P_h(C=c)$. We can then plug in the opinions of the sub-juries $ P_h(B=b) $ and $ P_h(C=c) $ in place of $ P_i(B=b) $ and $P_i(ğ¶=ğ‘$\|$B=1)$ in $\eqref{4}$:

$$
\label{5}
    \begin{aligned} 
    P_h(A)    =~ &P_h(B=0) P(A|B=0) \cr
                &\begin{aligned}
                   +~ P_h(B=1)  &\sum_{câ‰¥0} P_h(C=c)  \cr
                                &Ã— P(A|B=1,C=c)
                \end{aligned}
    \end{aligned}
    \tag{5}
$$


This gives us us the posterior belief of the meta-reasoner $ P_h(A) $ as a function of the prior probability function $ P $ and the evidence from the sub-juries $ P_h(B=b) $ and $ P_h(C=c) $. 

Using the shorthand $ F[P, P_h(B=b), P_h(C=c)] $ to refer to the formula in $\eqref{5}$, we illustrated this calculation in the chart below:

<img src="/assets/images/distributed-bayesian-reasoning/argument-thread-with-formulas.svg"
     alt="Argument Thread"
     style="display: block; margin-left: auto; margin-right: auto; width: 700px" />


To show a sample calculation, suppose we obtain the following probabilities for users that have voted on ğ´ and ğµ, and ğ¶.

|ğµ  |ğ¶     |  ğ‘ƒ(ğ´\|ğµ,ğ¶)
| --|----- | ----        
| 0 | -1   | 50% 
| 1 | 0    | 80% 
| 1 | 1    | 65% 

And suppose that the beliefs from the sub-juries are $P_h(B=1)=80\\%$ and $P_h(ğ¶=1) = 60\\%$. Plugging these into $\eqref{5}$:

$$
    \begin{aligned} 
    P_h(A=1) = ~ &(1-80\\%)Ã—50\\% \cr
                &+ 80\\%Ã—(1-60\\%)Ã—80\\% \cr
                &+ 80\\%Ã—60\\%Ã—65\\%\cr \cr
                = ~ &66.8\\%
    \end{aligned}
$$

Intuitively, this result reflects the fact that, although $B$ is an effective argument ($P(A=1 \vert B=1)=80\\%$) and the sub-jury mostly accepts it ($P_h(B=1)=80\\%$), ğ¶ is a fairly effective counter-argument ($P(A=1 \vert B=1,C=1) = 65\\%$).


## Formula for Long Threads

To generalize $\eqref{5}$, we first rewrite it in the more easily-generalizable form:

$$
    \begin{aligned} 
             P_h(A) =  ~ &\sum_{bâ‰¥0} P_h(B=b) Ã— \textbf{ if } b=0 \textbf{ then } P(A|B=0) \textbf{ else }   \cr
               &~~ \sum_{câ‰¥0} P_h(C=c|B=1) Ã— P(A|B=1,C=c)\cr \cr
    \end{aligned}
$$

Now suppose underneath the claim Î± there is a thread with ğ‘› premises $ Î² = \{Î²_1, Î²_2, ... ,Î²_n\} $. Then:

$$
\label{6}
\begin{aligned} 
 P_h&(Î±=1) =\cr
    &\sum_{b_1â‰¥0} P_h(Î²_1=b_1) Ã— \textbf{ if } b_1=0 \textbf{ then } P(Î±=1|Î²_1=0) \textbf{ else }   \cr
    &~~\sum_{b_2â‰¥0} P_h(Î²_2=b_2) Ã— \textbf{ if } b_2=0 \textbf{ then } P(Î±=1|Î²_1=1, Î²_2=0) \textbf{ else }   \cr
    &\space\space\space...  \cr
    &\space\space\space\space\space\space \sum_{b_nâ‰¥0} P_h(Î²_n=b_n) Ã— P(Î±=1|Î²_1=1, Î²_2=1, ... , Î²_n=1)
\end{aligned}
\tag{6}
$$

Note this function $P_h$ is recursive. The recursion terminates when it reaches a **terminal claim** in the argument graph -- a claim without any premise arguments underneath it -- in which case Î² will be âˆ… and the function will therefore return 

    
$$
    P_i(Î±=1) = P(Î±=1 \vert âˆ…) = P(Î±=1)
$$

or the raw probability that a user accepts the terminal claim Î±.


## Next in this Series

We can now calculate the posterior beliefs of the meta-reasoner for fairly complex argument trees, comprising arbitrarily long argument threads, and arbitrarily deep nesting of juries and sub-juries. But what about cases where there are multiple premise arguments under a claim (each starting a thread), or even multiple [warrant arguments](/argument-model#warrant-arguments) under a premise argument?

We will address this issue, as well as the problem of sampling error, in the article on [Bayesian Averaging](/distributed-bayesian-reasoning-bayesian-averaging)

## Appendix


### Derivation 1


Let's define a new variable $J$ that indicates that a user has participated in the sub-jury and voted on G:

$$
    J â‰ G â‰¥ 0
$$

Note also that all participants in the sub-jury vote on ğµ, so

$$
    J âŸ¹ G â‰¥ 0 âŸ¹ B â‰¥ 0
$$

Our causal assumptions are that: 

1. simply voting on ğµ (and thus being informed of the arguments for/against ğ´) effects probability of accepting ğ´ and.

2. ğµ is the **only** variable that directly effects the probability of accepting ğ´ (the [conditional-independence](#conditional-independence# assumption).

These assumptions give us this causal graph:

$$
    ğ½ â†’ ğµ â†’ ğ´
$$

We previously defined 

$$
    P_h(B) = P(B|G â‰¥ 0) = P(B|J)
$$

We now want to calculate

$$
    P_h(A) = P_i(A|do(J))
$$

That is, the probability that a user who voted on B **would** accept $A$ if they voted on $G$ (even though no user has actually done so).

$$
\begin{aligned} 
    P_h(A) &= P_i(A|do(J))  \cr  \cr
            &= \sum_{b} P_i(B=b|J) &&\text{front-door adj. formula} \cr
            &~~~~~~~~~Ã— \sum_{j'}P_i(A|J=j',B=b)P_i(J=j') \cr \cr
            &= \sum_{b} P_i(B=b|J)P_i(A|B=b) && \cr \cr
            &= \sum_{bâ‰¥0} P(B=b|J)P(A|B=b) &&\text{definition of } P_i \cr \cr
            &= \sum_{bâ‰¥0} P_h(B=b)P(A|B=b) &&\text{definition of } P_h \cr \cr
\end{aligned}
$$

Which is $\eqref{3}$.


### Derivation 2

Given this definition for $P_i$

$$
\begin{aligned} 
    P_i(A)    = P(A | B=0 âˆ¨ (B=1 âˆ§ Câ‰¥0) )
\end{aligned}
$$

We can rewrite $P_i(A)$ as:

$$
\begin{aligned} 
    P_i(A)&=  \cr
            &\begin{aligned}
                &= P_i(A|bâ‰¥0) ~~~~ &&\text{definition of }P_i\cr \cr
                &= \sum_{b \geq 0} P_i(B=b)P_i(A|B=b)  ~~~~&&\text{law of total prob}\cr \cr
                &= P_i(B=0)P(A|B=0) ~~~~&&\text{definition of }P_i \cr
                &\space\space\space\space+ P_i(B=1)P(A \vert B=1, C â‰¥ 0)\cr \cr
                &=    P_i(B=0) P(A|B=0)
            \end{aligned} \cr
            &\space\space\space\space\begin{aligned}
                    + P_i(B=1) &\sum_{câ‰¥0} P(C=c \vert B=1) ~~~~&&\text{law of total prob.} \cr
                          &Ã— P(A \vert B=1,C=c) 
            \end{aligned} 
\end{aligned}
$$



Which is $\eqref{4}$.


<!-- NOTES

Concepts from Argument Model:
    joint significance
    argument threads are dialogs
        presume acceptance
    definition of claim
    sample argument graph

-->
